#!/usr/bin/env python3
"""
Meta-Grade Async SRE Agent - Production Implementation
Monitors logs, matches errors against knowledge base, and auto-fixes issues
Supports: GPT-4, Claude, Gemini, Local Ollama
"""

import time
import json
import subprocess
import os
import sys
from datetime import datetime
from pathlib import Path

# Try to import requests, fallback gracefully
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    print("âš ï¸  'requests' not installed. Install with: pip3 install requests")
    HAS_REQUESTS = False

# CONFIGURATION
PROJECT_ROOT = Path(__file__).parent.parent.parent
LOG_FILES = [
    str(PROJECT_ROOT / "server.log"),
    str(PROJECT_ROOT / "backend" / "error.log")
]
KNOWLEDGE_BASE_PATH = str(PROJECT_ROOT / "backend" / "config" / "common_failures.json")
CONFIG_PATH = str(PROJECT_ROOT / "backend" / "config" / "agent_config.toml")

# LLM Configuration (uses Gemini by default from .env)
LLM_PROVIDER = os.getenv("SRE_LLM_PROVIDER", "gemini")  # gemini, openai, claude, ollama
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

SAFETY_MODE = os.getenv("SRE_SAFETY_MODE", "true").lower() == "true"
MAX_FIXES_PER_HOUR = int(os.getenv("SRE_MAX_FIXES_PER_HOUR", "10"))

class MetaGradeSREAgent:
    def __init__(self):
        self.knowledge_base = self.load_knowledge_base()
        self.fixes_applied = []
        self.fixes_this_hour = 0
        self.last_hour_reset = time.time()
        
    def load_knowledge_base(self):
        """Load the 350+ error knowledge base"""
        try:
            with open(KNOWLEDGE_BASE_PATH, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âŒ Knowledge base not found at {KNOWLEDGE_BASE_PATH}")
            return {"meta_knowledge_base": {"categories": {}}}
    
    def read_recent_logs(self, lines=100):
        """Read last N lines from all log files"""
        combined_logs = ""
        for log_file in LOG_FILES:
            if os.path.exists(log_file):
                try:
                    result = subprocess.run(
                        ['tail', '-n', str(lines), log_file],
                        capture_output=True,
                        text=True,
                        timeout=5
                    )
                    if result.stdout:
                        combined_logs += f"\n=== {log_file} ===\n{result.stdout}"
                except Exception as e:
                    print(f"âš ï¸  Error reading {log_file}: {e}")
        return combined_logs
    
    def detect_issues(self, logs):
        """Match log patterns against knowledge base"""
        detected = []
        kb = self.knowledge_base.get("meta_knowledge_base", {}).get("categories", {})
        
        # Check each category
        for category, errors in kb.items():
            for error_def in errors:
                pattern = error_def.get("error") or error_def.get("code") or error_def.get("issue", "")
                if pattern and pattern in logs:
                    detected.append({
                        "category": category,
                        "error": error_def,
                        "severity": error_def.get("severity", "MEDIUM")
                    })
        
        return detected
    
    def query_llm_sre(self, logs, detected_issues):
        """Send logs to LLM for analysis and fix generation"""
        
        # Build prompt
        prompt = f"""You are Meta-SRE-Bot, an autonomous Site Reliability Engineer.

CONTEXT:
You are analyzing production logs and must propose fixes based on known error patterns.

DETECTED ISSUES:
{json.dumps(detected_issues, indent=2)}

RECENT LOGS:
{logs[-2000:]}  

YOUR TASK:
Analyze the logs and propose a specific fix. Output ONLY valid JSON in this exact format:

{{
  "detected_issue": "Brief description",
  "severity": "HIGH|MEDIUM|LOW|CRITICAL",
  "root_cause_analysis": "Detailed root cause",
  "recommended_fix": {{
    "file": "path/to/file.js",
    "action": "add_circuit_breaker|increase_timeout|add_retry_logic|add_validation",
    "code_snippet": "actual code to add",
    "line_number": 0
  }},
  "confidence_score": 0.0-1.0,
  "auto_fix_safe": true|false
}}

IMPORTANT: Output ONLY the JSON, no markdown, no explanation."""

        try:
            if LLM_PROVIDER == "gemini" and GEMINI_API_KEY:
                return self.query_gemini(prompt)
            elif LLM_PROVIDER == "openai" and OPENAI_API_KEY:
                return self.query_openai(prompt)
            else:
                # Fallback to rule-based fixes
                return self.generate_rule_based_fix(detected_issues)
        except Exception as e:
            print(f"âš ï¸  LLM query failed: {e}")
            return self.generate_rule_based_fix(detected_issues)
    
    def query_gemini(self, prompt):
        """Query Google Gemini API"""
        if not HAS_REQUESTS or not GEMINI_API_KEY:
            return None
            
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}"
        
        response = requests.post(url, json={
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {"temperature": 0.1}
        }, timeout=30)
        
        if response.status_code == 200:
            text = response.json()['candidates'][0]['content']['parts'][0]['text']
            # Extract JSON from response
            if '{' in text:
                json_str = text[text.find('{'):text.rfind('}')+1]
                return json.loads(json_str)
        return None
    
    def query_openai(self, prompt):
        """Query OpenAI API"""
        if not HAS_REQUESTS or not OPENAI_API_KEY:
            return None
            
        url = "https://api.openai.com/v1/chat/completions"
        headers = {"Authorization": f"Bearer {OPENAI_API_KEY}"}
        
        response = requests.post(url, headers=headers, json={
            "model": "gpt-4",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.1
        }, timeout=30)
        
        if response.status_code == 200:
            text = response.json()['choices'][0]['message']['content']
            if '{' in text:
                json_str = text[text.find('{'):text.rfind('}')+1]
                return json.loads(json_str)
        return None
    
    def generate_rule_based_fix(self, detected_issues):
        """Fallback: Generate fix based on knowledge base rules"""
        if not detected_issues:
            return None
            
        # Take the highest severity issue
        issue = max(detected_issues, key=lambda x: 
            {"CRITICAL": 4, "HIGH": 3, "MEDIUM": 2, "LOW": 1}.get(x["severity"], 0))
        
        error_def = issue["error"]
        
        return {
            "detected_issue": error_def.get("issue", "Unknown"),
            "severity": issue["severity"],
            "root_cause_analysis": f"Matched known pattern: {error_def.get('error', '')}",
            "recommended_fix": {
                "file": "backend/server.js",
                "action": "add_error_handler",
                "code_snippet": f"// Fix: {error_def.get('fix', 'Review logs')}",
                "line_number": 0
            },
            "confidence_score": 0.7,
            "auto_fix_safe": False  # Conservative for rule-based
        }
    
    def apply_fix(self, fix_data):
        """Apply the proposed fix"""
        file_path = PROJECT_ROOT / fix_data["recommended_fix"]["file"]
        
        if not file_path.exists():
            print(f"âŒ File not found: {file_path}")
            return False
        
        # Create backup
        backup_path = f"{file_path}.bak.{int(time.time())}"
        subprocess.run(['cp', str(file_path), backup_path])
        print(f"ğŸ“¦ Backup created: {backup_path}")
        
        # Append fix (safest approach)
        code = fix_data["recommended_fix"]["code_snippet"]
        with open(file_path, 'a') as f:
            f.write(f"\n\n// AUTO-FIX by Meta-SRE Agent - {datetime.now()}\n")
            f.write(code)
            f.write("\n")
        
        print(f"âœ… Applied fix to {file_path}")
        
        # Log the fix
        self.fixes_applied.append({
            "timestamp": datetime.now().isoformat(),
            "file": str(file_path),
            "issue": fix_data["detected_issue"],
            "confidence": fix_data["confidence_score"]
        })
        
        return True
    
    def run_async_loop(self, duration_minutes=300):
        """Main monitoring loop"""
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Meta-Grade Async SRE Agent - JoBika Production Monitor â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ Mission: Monitor logs and auto-fix production issuess
â±ï¸  Duration: {duration_minutes} minutes
ğŸ¤– LLM: {LLM_PROVIDER.upper()} {'âœ…' if GEMINI_API_KEY or OPENAI_API_KEY else 'âŒ (rule-based fallback)'}
ğŸ›¡ï¸  Safety Mode: {'ON' if SAFETY_MODE else 'OFF'}
ğŸ“Š Max Fixes/Hour: {MAX_FIXES_PER_HOUR}
        """)
        
        start_time = time.time()
        iteration = 0
        
        while (time.time() - start_time) < (duration_minutes * 60):
            iteration += 1
            
            # Reset hourly fix counter
            if time.time() - self.last_hour_reset > 3600:
                self.fixes_this_hour = 0
                self.last_hour_reset = time.time()
            
            print(f"\n{'='*60}")
            print(f"ğŸ” Iteration {iteration} - {datetime.now().strftime('%H:%M:%S')}")
            print(f"   Fixes this hour: {self.fixes_this_hour}/{MAX_FIXES_PER_HOUR}")
            
            # Step 1: Read logs
            logs = self.read_recent_logs()
            
            # Step 2: Detect issues
            detected = self.detect_issues(logs)
            
            if not detected:
                print("âœ… No issues detected. System healthy.")
                time.sleep(60)
                continue
            
            print(f"âš ï¸  Detected {len(detected)} potential issues:")
            for issue in detected[:3]:  # Show top 3
                print(f"   - [{issue['severity']}] {issue['error'].get('issue', 'Unknown')}")
            
            # Step 3: Analyze with LLM
            if self.fixes_this_hour >= MAX_FIXES_PER_HOUR:
                print("â¸ï¸  Max fixes per hour reached. Monitoring only...")
                time.sleep(300)  # Wait 5 minutes
                continue
            
            fix_proposal = self.query_llm_sre(logs, detected)
            
            if not fix_proposal:
                print("âš ï¸  No fix proposal generated")
                time.sleep(60)
                continue
            
            print(f"\nğŸ’¡ Proposed Fix:")
            print(f"   Issue: {fix_proposal['detected_issue']}")
            print(f"   Confidence: {fix_proposal['confidence_score']:.0%}")
            print(f"   Action: {fix_proposal['recommended_fix']['action']}")
            
            # Step 4: Apply fix if safe
            should_apply = (
                fix_proposal['confidence_score'] > 0.85 and
                fix_proposal.get('auto_fix_safe', False)
            )
            
            if SAFETY_MODE and should_apply:
                print("\nâš ï¸  Review required:")
                try:
                    input("Press Enter to apply fix, Ctrl+C to skip... ")
                except KeyboardInterrupt:
                    print("\nâ­ï¸  Skipped")
                    time.sleep(60)
                    continue
            
            if should_apply or not SAFETY_MODE:
                if self.apply_fix(fix_proposal):
                    self.fixes_this_hour += 1
                    print("ğŸ”§ Fix applied successfully!")
                    time.sleep(60)  # Stabilization time
            else:
                print("ğŸ‘¤ Manual review required (low confidence)")
                # Save for human review
                review_dir = PROJECT_ROOT / "backend" / "incidents"
                review_dir.mkdir(exist_ok=True)
                with open(review_dir / f"incident_{int(time.time())}.json", 'w') as f:
                    json.dump(fix_proposal, f, indent=2)
            
            time.sleep(60)
        
        # Final report
        print(f"\n{'='*60}")
        print(f"âœ… Monitoring complete!")
        print(f"ğŸ“Š Total fixes applied: {len(self.fixes_applied)}")
        
        # Save report
        report_path = PROJECT_ROOT / "backend" / "sre_report.json"
        with open(report_path, 'w') as f:
            json.dump({
                "total_fixes": len(self.fixes_applied),
                "fixes": self.fixes_applied,
                "duration_minutes": duration_minutes,
                "timestamp": datetime.now().isoformat()
            }, f, indent=2)
        print(f"ğŸ“ Report saved: {report_path}")

if __name__ == "__main__":
    duration = int(sys.argv[1]) if len(sys.argv) > 1 else 300
    agent = MetaGradeSREAgent()
    
    try:
        agent.run_async_loop(duration)
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Agent stopped by user")
        sys.exit(0)
